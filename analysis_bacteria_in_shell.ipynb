{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39ad49c8",
      "metadata": {
        "id": "39ad49c8"
      },
      "source": [
        "# Bacteria in shell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5977e982",
      "metadata": {
        "id": "5977e982"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import io, ndimage as ndi, signal\n",
        "from skimage import io as skio, color, filters, segmentation, feature, measure, morphology, transform, draw\n",
        "from skimage.filters import median as skimage_filters_median, threshold_otsu, threshold_yen, threshold_multiotsu, gaussian\n",
        "from skimage.io import imread\n",
        "from skimage.measure import regionprops, regionprops_table, label\n",
        "from skimage.morphology import erosion, dilation, square, disk\n",
        "from skimage.segmentation import clear_border, watershed\n",
        "from skimage.feature import peak_local_max\n",
        "from skimage.draw import circle_perimeter\n",
        "from skimage.color import label2rgb\n",
        "import random\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import pytz\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import glob\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import re\n",
        "import platform\n",
        "from skimage.io import imread as skio_imread\n",
        "\n",
        "# --- User configuration (edit these two paths) ---\n",
        "DATA_DIR = r\"path/to/data_directory\"       # folder containing your image folders\n",
        "OUTPUT_DIR = r\"path/to/output_directory\"   # where CSV/Summary and exported crops will be saved\n",
        "\n",
        "# -------------------- Metadata --------------------\n",
        "CodeName = \"Bacteria_in_shell\"\n",
        "Version = \"1.4.20250405_shellThicknessErode\"\n",
        "Python_version = platform.python_version()\n",
        "exp_id = \"XB803\"\n",
        "\n",
        "# -------------------- IO paths --------------------\n",
        "base_dir = DATA_DIR\n",
        "folder_name_startwith = \"folder_prefix\"\n",
        "save_path_fig = os.path.join(OUTPUT_DIR, \"exports\")\n",
        "save_path_data_sheet = os.path.join(OUTPUT_DIR, \"results\") + os.sep\n",
        "os.makedirs(save_path_fig, exist_ok=True)\n",
        "os.makedirs(save_path_data_sheet, exist_ok=True)\n",
        "\n",
        "# Cropped images go here (single folder)\n",
        "cropped_out_dir = os.path.join(save_path_fig, f\"{folder_name_startwith}_cropped\")\n",
        "os.makedirs(cropped_out_dir, exist_ok=True)\n",
        "\n",
        "# -------------------- Presets --------------------\n",
        "image_export = 0  # legacy matplotlib export; keep 0 because we now use fast crop export\n",
        "only_one_image = 0\n",
        "only_one_image_id = list(range(0, 1))\n",
        "export_in_svg = 0\n",
        "\n",
        "background_correction = 1  # 1 = apply background subtraction for DAPI; 0 = skip\n",
        "\n",
        "channel_num = 4\n",
        "num_steps = 4\n",
        "channel_name = [\"DAPI\", \"FITC-agarose\", \"AF647-alginate\", \"PC\"]\n",
        "label_channel = 1          # labels come from FITC-agarose\n",
        "intensity_channel = 0      # measure intensity from DAPI\n",
        "analysis_list = [1, 1, 0, 0]\n",
        "binarization_list = [0, 1, 0, 0]\n",
        "threshold_list = [0, threshold_otsu, 0, 0]\n",
        "signal_threshold_list = [0, 0, 0, 0]\n",
        "gaussian_sigma_list = [1, 1, 1, 1]\n",
        "ero = 5\n",
        "dil = 5\n",
        "watershed_list = [0, 1, 0, 0]\n",
        "watershed_footprint = (3, 3)\n",
        "circularity_threshold_list = [0, 0.9, 0, 0]\n",
        "area_threshold_list = [0, 7000, 0, 0]\n",
        "upper_area_threshold_list = [100000, 10000, 100000, 100000]\n",
        "regionprops_tuple = (\n",
        "    \"label\", \"centroid\", \"area\", \"intensity_mean\",\n",
        "    \"major_axis_length\", \"minor_axis_length\", \"solidity\"\n",
        ")\n",
        "\n",
        "timestamp_short = datetime.utcnow().replace(tzinfo=pytz.timezone(\"UTC\"))\\\n",
        "    .astimezone(pytz.timezone(\"Asia/Tokyo\")).strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "# ---- Fast conditional CROP export settings ----\n",
        "INTENSITY_CUTOFF = 84.53        # threshold for inside-shell DAPI mean\n",
        "CROP_EXPORT = True             # turn on fast crop exports\n",
        "CROP_PAD = 10                  # pixels of padding around each shell crop\n",
        "MAX_CROPS_PER_FIELD = 50       # cap per f-id to avoid too many files\n",
        "MAX_CROP_WIDTH = 800           # downscale wide crops; set None to keep original\n",
        "JPEG_QUALITY = 85              # 70–90 is a good balance\n",
        "\n",
        "# ---- NEW: shell thickness erosion settings ----\n",
        "# 445 pixels correspond to 275 µm\n",
        "UM_PER_PIXEL = 275.0 / 445.0   # ≈ 0.618 µm / pixel\n",
        "# >>> Set this to the thickness (µm) you want to erode away from the shell mask:\n",
        "SHELL_EROSION_UM = 6.1         # e.g. 5.0; set 0.0 to disable additional erosion\n",
        "\n",
        "# We need raw images for these channels even if analysis_list says 0\n",
        "# 0:DAPI, 1:FITC-agarose, 3:PC\n",
        "REQUIRED_RAW_FOR_EXPORT = {0, 1, 3}\n",
        "\n",
        "# -------------------- Helpers --------------------\n",
        "def filter_labels_by_circularity(label_img, circularity_threshold=0.0):\n",
        "    selected_labels = []\n",
        "    for prop in regionprops(label_img):\n",
        "        area = prop.area\n",
        "        perimeter = prop.perimeter if prop.perimeter > 0 else 1\n",
        "        circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
        "        if circularity >= circularity_threshold:\n",
        "            selected_labels.append(prop.label)\n",
        "    return np.isin(label_img, selected_labels)\n",
        "\n",
        "def filter_labels_by_area(label_img, area_min=0, area_max=np.inf):\n",
        "    label_img = label_img.astype(np.int32)\n",
        "    selected_labels = [\n",
        "        prop.label for prop in regionprops(label_img)\n",
        "        if area_min <= prop.area <= area_max\n",
        "    ]\n",
        "    return np.isin(label_img, selected_labels)\n",
        "\n",
        "def basic_binarization(img, gaussian_sigma, threshold_method, binarization_apply, signal_threshold):\n",
        "    if binarization_apply == 0:\n",
        "        return (img >= signal_threshold).astype(bool)\n",
        "    img = skimage_filters_median(img)\n",
        "    img = gaussian(img, sigma=gaussian_sigma)\n",
        "    img_thresh = threshold_method(img)\n",
        "    img = (img >= img_thresh)\n",
        "    img = erosion(img, square(ero))\n",
        "    img = dilation(img, square(dil))\n",
        "    img = ndi.binary_fill_holes(img)\n",
        "    img = clear_border(img)\n",
        "    return img.astype(bool)\n",
        "\n",
        "def apply_watershed(img):\n",
        "    distance = ndi.distance_transform_edt(img)\n",
        "    local_maxi = peak_local_max(distance,\n",
        "                                footprint=np.ones(watershed_footprint),\n",
        "                                labels=img.astype(bool))\n",
        "    local_maxi_mask = np.zeros_like(distance, dtype=bool)\n",
        "    if local_maxi.size > 0:\n",
        "        local_maxi_mask[tuple(local_maxi.T)] = True\n",
        "    markers, _ = ndi.label(local_maxi_mask)\n",
        "    label_img = watershed(-distance, markers, mask=img.astype(bool))\n",
        "    return label_img.astype(np.int32)\n",
        "\n",
        "def subtract_median_background(dapi_img, fitc_binarized):\n",
        "    # background = DAPI outside FITC-agarose mask\n",
        "    background_mask = (fitc_binarized == False)\n",
        "    background_values = dapi_img[background_mask]\n",
        "    if len(background_values) == 0:\n",
        "        return dapi_img, 0.0\n",
        "    median_bkg = np.median(background_values)\n",
        "    corrected = dapi_img.astype(float) - median_bkg\n",
        "    corrected[corrected < 0] = 0\n",
        "    return corrected, median_bkg\n",
        "\n",
        "def rename_df_columns(\n",
        "    df, label_channel, intensity_channel, channel_name, background_correction\n",
        "):\n",
        "    label_prefix = f\"{channel_name[label_channel]}-mask\"\n",
        "    if background_correction == 1:\n",
        "        intensity_suffix = f\"{channel_name[intensity_channel]}_mean_intensity_corrected\"\n",
        "    else:\n",
        "        intensity_suffix = f\"{channel_name[intensity_channel]}_mean_intensity_raw\"\n",
        "    intensity_prefix = f\"{channel_name[label_channel]}-mask_{intensity_suffix}\"\n",
        "    rename_dict = {\n",
        "        \"label\":             f\"{label_prefix}_label\",\n",
        "        \"area\":              f\"{label_prefix}_area\",\n",
        "        \"centroid-0\":        f\"{label_prefix}_centroid-0\",\n",
        "        \"centroid-1\":        f\"{label_prefix}_centroid-1\",\n",
        "        \"major_axis_length\": f\"{label_prefix}_major_axis_length\",\n",
        "        \"minor_axis_length\": f\"{label_prefix}_minor_axis_length\",\n",
        "        \"solidity\":          f\"{label_prefix}_solidity\",\n",
        "        \"intensity_mean\":    intensity_prefix\n",
        "    }\n",
        "    df.rename(columns=rename_dict, inplace=True)\n",
        "    return df\n",
        "\n",
        "def _normalize_to_uint8(img):\n",
        "    # robust percentile stretch to uint8 for display\n",
        "    vmin, vmax = np.percentile(img, [1, 99])\n",
        "    if vmax <= vmin:\n",
        "        vmax = vmin + 1e-6\n",
        "    img_n = np.clip((img - vmin) / (vmax - vmin), 0, 1)\n",
        "    return (img_n * 255).astype(np.uint8)\n",
        "\n",
        "def _square_bbox_with_pad(bbox, pad, h, w):\n",
        "    # bbox: (minr, minc, maxr, maxc) -> square bbox with padding and clipping\n",
        "    minr, minc, maxr, maxc = bbox\n",
        "    hr = maxr - minr\n",
        "    wr = maxc - minc\n",
        "    side = max(hr, wr)\n",
        "    cr = (minr + maxr) // 2\n",
        "    cc = (minc + maxc) // 2\n",
        "    half = side // 2\n",
        "    r0 = cr - half\n",
        "    r1 = r0 + side\n",
        "    c0 = cc - half\n",
        "    c1 = c0 + side\n",
        "    r0 -= pad; r1 += pad; c0 -= pad; c1 += pad\n",
        "    r0 = max(0, r0); c0 = max(0, c0)\n",
        "    r1 = min(h, r1); c1 = min(w, c1)\n",
        "    return int(r0), int(c0), int(r1), int(c1)\n",
        "\n",
        "# -------------------- Discover folders --------------------\n",
        "folder_names = sorted([\n",
        "    d for d in os.listdir(base_dir)\n",
        "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(folder_name_startwith)\n",
        "])\n",
        "\n",
        "# -------------------- Main loop --------------------\n",
        "for folder_name in folder_names:\n",
        "    parts = folder_name.rsplit(\"_Day\", 1)\n",
        "    mouse_info = parts[0]\n",
        "    day_info = \"Day\" + parts[1]\n",
        "    sample_id = f\"{mouse_info}_{day_info}\"\n",
        "    main_path = os.path.join(base_dir, folder_name)\n",
        "\n",
        "    tif_files = sorted([f for f in os.listdir(main_path) if f.lower().endswith('.tif')])\n",
        "\n",
        "    summary_lines = [\n",
        "        \"\",\n",
        "        f\"Code: {CodeName}_Ver. {Version}\",\n",
        "        f\"Python_version: {Python_version}\",\n",
        "        f\"Date: {datetime.now(pytz.timezone('Asia/Tokyo')).strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "        f\"Experiment ID: {exp_id}\",\n",
        "        f\"Sample ID: {sample_id}\",\n",
        "        f\"File Path: {main_path}\",\n",
        "        f\"Number of TIFF Files: {len(tif_files)}\",\n",
        "        f\"Number of Channels per Field: {channel_num}\",\n",
        "        f\"Total Steps (Raw & Processed Images): {num_steps}\",\n",
        "        f\"Channel Names: {', '.join(channel_name)}\",\n",
        "        f\"Gaussian Sigma Values: {gaussian_sigma_list}\",\n",
        "        f\"Erosion Kernel Size: {ero}\",\n",
        "        f\"Dilation Kernel Size: {dil}\",\n",
        "        f\"Watershed Application per Channel: {watershed_list}\",\n",
        "        f\"Lower Area Thresholds per Channel: {area_threshold_list}\",\n",
        "        f\"Upper Area Thresholds per Channel: {upper_area_threshold_list}\",\n",
        "        f\"Circularity Threshold: {circularity_threshold_list}\",\n",
        "        f\"Binarization Settings per Channel: {binarization_list}\",\n",
        "        f\"Binarization Methods: {threshold_list}\",\n",
        "        f\"Signal Thresholds per Channel: {signal_threshold_list}\",\n",
        "        f\"Labels for Region Properties: {channel_name[label_channel]}\",\n",
        "        f\"Extended Properties for Region Analysis: {', '.join(regionprops_tuple)}\",\n",
        "        f\"INTENSITY_CUTOFF (inside shells): {INTENSITY_CUTOFF}\",\n",
        "        f\"SHELL_EROSION_UM: {SHELL_EROSION_UM}\",\n",
        "        f\"UM_PER_PIXEL: {UM_PER_PIXEL}\",\n",
        "        \"\",\n",
        "        \"List of TIFF Files:\"\n",
        "    ]\n",
        "    summary_lines.extend(tif_files)\n",
        "    summary_lines.append(\"\")\n",
        "\n",
        "    if len(tif_files) == 0:\n",
        "        print(f\"No TIFF files found in {main_path}. Skipping...\")\n",
        "        summary_filename = f\"{save_path_data_sheet}{sample_id}_{timestamp_short}_Summary.txt\"\n",
        "        with open(summary_filename, 'a') as f:\n",
        "            for line in summary_lines:\n",
        "                f.write(line + '\\n')\n",
        "        continue\n",
        "\n",
        "    # --- Parse and group by f-id; map d -> channel ---\n",
        "    # filenames like: \"..._A01fXXdYY.tif\"  (XX = 0..199; YY in {0,1,3,4})\n",
        "    f_d_regex = re.compile(r\"_A\\d{2}f(?P<f>\\d{1,3})d(?P<d>[0134])(?:\\.tif|_)\", re.IGNORECASE)\n",
        "\n",
        "    # Map each d to your channel index order:\n",
        "    # d=0 -> channel 0 (DAPI)\n",
        "    # d=1 -> channel 1 (FITC-agarose)\n",
        "    # d=3 -> channel 2 (AF647-alginate)\n",
        "    # d=4 -> channel 3 (PC)\n",
        "    d_to_channel = {0: 0, 1: 1, 3: 2, 4: 3}\n",
        "    channel_to_d  = {v: k for k, v in d_to_channel.items()}\n",
        "\n",
        "    files_by_f = {}\n",
        "    for fname in tif_files:\n",
        "        m = f_d_regex.search(fname)\n",
        "        if not m:\n",
        "            continue\n",
        "        f_id = int(m.group(\"f\"))\n",
        "        d_id = int(m.group(\"d\"))\n",
        "        files_by_f.setdefault(f_id, {})[d_id] = fname\n",
        "\n",
        "    sorted_f_ids = sorted(files_by_f.keys())\n",
        "    if not sorted_f_ids:\n",
        "        print(f\"[{sample_id}] No files matched the f/d pattern. Check filenames.\")\n",
        "        summary_filename = f\"{save_path_data_sheet}{sample_id}_{timestamp_short}_Summary.txt\"\n",
        "        with open(summary_filename, 'a') as f:\n",
        "            for line in summary_lines:\n",
        "                f.write(line + '\\n')\n",
        "        continue\n",
        "\n",
        "    # Which f-ids to process\n",
        "    if only_one_image:\n",
        "        target_f_ids = [i for i in only_one_image_id if i in files_by_f]\n",
        "        if not target_f_ids:\n",
        "            print(f\"[{sample_id}] Requested only_one_image_id not found. Skipping sample.\")\n",
        "            summary_filename = f\"{save_path_data_sheet}{sample_id}_{timestamp_short}_Summary.txt\"\n",
        "            with open(summary_filename, 'a') as f:\n",
        "                for line in summary_lines:\n",
        "                    f.write(line + '\\n')\n",
        "            continue\n",
        "    else:\n",
        "        target_f_ids = sorted_f_ids\n",
        "\n",
        "    # -------- Analysis over f-ids --------\n",
        "    df_all = pd.DataFrame()\n",
        "\n",
        "    for image_pos_id in target_f_ids:  # image_id equals the \"f\" number\n",
        "        file_dict = files_by_f[image_pos_id]\n",
        "\n",
        "        required_ds = {0, 1, 3, 4}\n",
        "        missing = required_ds - set(file_dict.keys())\n",
        "        if missing:\n",
        "            print(f\"[{sample_id}] f={image_pos_id}: missing d={sorted(list(missing))}, skipping this set.\")\n",
        "            continue\n",
        "\n",
        "        # Read one file to get shape (prefer DAPI)\n",
        "        first_file_for_shape = file_dict.get(0, list(file_dict.values())[0])\n",
        "        sample_img_shape = imread(os.path.join(main_path, first_file_for_shape), plugin='pil').shape\n",
        "        imgs = np.zeros((channel_num, num_steps, *sample_img_shape), dtype=np.float32)\n",
        "\n",
        "        # 1) Read all channels for this f-id (by channel index 0..3)\n",
        "        read_error = False\n",
        "        for j in range(channel_num):\n",
        "            need_this = analysis_list[j] or (CROP_EXPORT and j in REQUIRED_RAW_FOR_EXPORT)\n",
        "            if not need_this:\n",
        "                continue\n",
        "            d_needed = channel_to_d[j]\n",
        "            file_to_read = file_dict[d_needed]\n",
        "            try:\n",
        "                imgs[j, 0] = imread(os.path.join(main_path, file_to_read), plugin='pil')\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR reading file {file_to_read} for f={image_pos_id}: {e}\")\n",
        "                read_error = True\n",
        "                break\n",
        "\n",
        "        if read_error:\n",
        "            continue\n",
        "\n",
        "        # 2) Binarize the FITC channel (label_channel == 1)\n",
        "        if analysis_list[1] and binarization_list[1]:\n",
        "            img_bin_fitc = basic_binarization(\n",
        "                imgs[1, 0],\n",
        "                gaussian_sigma_list[1],\n",
        "                threshold_list[1],\n",
        "                binarization_list[1],\n",
        "                signal_threshold_list[1]\n",
        "            )\n",
        "            imgs[1, 1] = img_bin_fitc\n",
        "        else:\n",
        "            imgs[1, 1] = imgs[1, 0] > 0\n",
        "\n",
        "        # 3) Background subtraction on DAPI (using FITC mask)\n",
        "        median_bkg = 0.0\n",
        "        if background_correction == 1 and analysis_list[0]:\n",
        "            corrected, median_bkg = subtract_median_background(imgs[0, 0], imgs[1, 1])\n",
        "            imgs[0, 0] = corrected\n",
        "\n",
        "        # 4) Label on FITC + filtering\n",
        "        if analysis_list[1] and binarization_list[1]:\n",
        "            if watershed_list[1]:\n",
        "                imgs[1, 2] = apply_watershed(imgs[1, 1])\n",
        "            else:\n",
        "                imgs[1, 2], _ = ndi.label(imgs[1, 1])\n",
        "            filtered_by_area = filter_labels_by_area(\n",
        "                imgs[1, 2],\n",
        "                area_min=area_threshold_list[1],\n",
        "                area_max=upper_area_threshold_list[1]\n",
        "            )\n",
        "            label_img_area_filtered, _ = ndi.label(filtered_by_area.astype(bool))\n",
        "            imgs[1, 3] = filter_labels_by_circularity(\n",
        "                label_img_area_filtered,\n",
        "                circularity_threshold=circularity_threshold_list[1]\n",
        "            ).astype(bool)\n",
        "            imgs[1, 3], _ = ndi.label(imgs[1, 3])\n",
        "        else:\n",
        "            imgs[1, 3] = np.zeros_like(imgs[1, 1])\n",
        "\n",
        "        # ==== NEW: erode shell labels by physical thickness (µm) ====\n",
        "        if SHELL_EROSION_UM > 0:\n",
        "            erode_pixels = int(math.ceil(SHELL_EROSION_UM / UM_PER_PIXEL))\n",
        "            if erode_pixels > 0:\n",
        "                shell_mask = imgs[1, 3] > 0\n",
        "                shell_mask_eroded = erosion(shell_mask, disk(erode_pixels))\n",
        "                # Re-label after erosion\n",
        "                imgs[1, 3], _ = ndi.label(shell_mask_eroded.astype(bool))\n",
        "\n",
        "        # 5) Regionprops (on FITC-selected labels; intensity from DAPI)\n",
        "        label_img = imgs[1, 3].astype(int)\n",
        "        df = pd.DataFrame(regionprops_table(\n",
        "            label_img,\n",
        "            intensity_image=imgs[0, 0],  # background-corrected if enabled\n",
        "            properties=regionprops_tuple\n",
        "        ))\n",
        "\n",
        "        df = rename_df_columns(\n",
        "            df,\n",
        "            label_channel,\n",
        "            intensity_channel,\n",
        "            channel_name=channel_name,\n",
        "            background_correction=background_correction\n",
        "        )\n",
        "\n",
        "        # add DAPI background median\n",
        "        bkg_col = f\"{channel_name[label_channel]}-mask_{channel_name[intensity_channel]}_background_median\"\n",
        "        df[bkg_col] = median_bkg\n",
        "\n",
        "        # Stamp IDs\n",
        "        df.insert(0, \"image_id\", image_pos_id)  # f number\n",
        "        df.insert(0, \"sample_id\", sample_id)\n",
        "        df.insert(0, \"exp_id\", exp_id)\n",
        "\n",
        "        df_all = pd.concat([df_all, df], ignore_index=True).fillna(0)\n",
        "\n",
        "        # ---- FAST CROPPED EXPORTS (no matplotlib) ----\n",
        "        intensity_col = f\"{channel_name[label_channel]}-mask_{channel_name[intensity_channel]}_mean_intensity_corrected\"\n",
        "        lbl_col = f\"{channel_name[label_channel]}-mask_label\"\n",
        "        hits = df[df[intensity_col] > INTENSITY_CUTOFF]\n",
        "        export_this_field = CROP_EXPORT and (len(hits) > 0)\n",
        "\n",
        "        if export_this_field:\n",
        "            H, W = label_img.shape\n",
        "\n",
        "            # Build bbox map\n",
        "            label_props = regionprops(label_img)\n",
        "            bbox_map = {p.label: p.bbox for p in label_props}\n",
        "\n",
        "            # Select hot labels and cap number\n",
        "            hot_labels = [int(x) for x in hits[lbl_col].tolist() if int(x) in bbox_map]\n",
        "            if len(hot_labels) > MAX_CROPS_PER_FIELD:\n",
        "                hot_labels = hot_labels[:MAX_CROPS_PER_FIELD]\n",
        "\n",
        "            for lbl in hot_labels:\n",
        "                lbl_int = int(lbl)\n",
        "\n",
        "                r0, c0, r1, c1 = _square_bbox_with_pad(bbox_map[lbl_int], CROP_PAD, H, W)\n",
        "\n",
        "                dapi_crop = imgs[0, 0][r0:r1, c0:c1]\n",
        "                fitc_crop = imgs[1, 0][r0:r1, c0:c1]\n",
        "                pc_crop   = imgs[3, 0][r0:r1, c0:c1] if imgs[3, 0].any() else None\n",
        "\n",
        "                # Normalize to 8-bit\n",
        "                dapi_u8 = _normalize_to_uint8(dapi_crop)\n",
        "                fitc_u8 = _normalize_to_uint8(fitc_crop)\n",
        "                if pc_crop is not None:\n",
        "                    pc_u8 = _normalize_to_uint8(pc_crop)\n",
        "\n",
        "                # Optional downscale\n",
        "                if MAX_CROP_WIDTH is not None:\n",
        "                    def _down(x):\n",
        "                        if x.shape[1] <= MAX_CROP_WIDTH:\n",
        "                            return x\n",
        "                        scale = MAX_CROP_WIDTH / x.shape[1]\n",
        "                        new_h = int(x.shape[0] * scale)\n",
        "                        return np.array(Image.fromarray(x).resize((MAX_CROP_WIDTH, new_h), Image.BILINEAR))\n",
        "                    dapi_u8 = _down(dapi_u8)\n",
        "                    fitc_u8 = _down(fitc_u8)\n",
        "                    if pc_crop is not None:\n",
        "                        pc_u8 = _down(pc_u8)\n",
        "\n",
        "                base = os.path.join(\n",
        "                    cropped_out_dir,\n",
        "                    f\"{sample_id}_f{image_pos_id:03d}_lbl{lbl_int}_{timestamp_short}\"\n",
        "                )\n",
        "                Image.fromarray(dapi_u8).save(base + \"_DAPI.jpg\", format=\"JPEG\", quality=JPEG_QUALITY, optimize=True)\n",
        "                Image.fromarray(fitc_u8).save(base + \"_FITC.jpg\", format=\"JPEG\", quality=JPEG_QUALITY, optimize=True)\n",
        "                if pc_crop is not None:\n",
        "                    Image.fromarray(pc_u8).save(base + \"_PC.jpg\",   format=\"JPEG\", quality=JPEG_QUALITY, optimize=True)\n",
        "\n",
        "        # (Optional) legacy matplotlib export — keep disabled for speed\n",
        "    # Save DataFrame for this folder\n",
        "    if not df_all.empty:\n",
        "        csv_filename = f\"{save_path_data_sheet}{sample_id}_{timestamp_short}.csv\"\n",
        "        df_all.to_csv(csv_filename, index=False)\n",
        "        print(f\"[{sample_id}] Saved: {csv_filename}\")\n",
        "    else:\n",
        "        print(f\"[{sample_id}] No data rows to save.\")\n",
        "\n",
        "    # Save summary\n",
        "    summary_filename = f\"{save_path_data_sheet}{sample_id}_{timestamp_short}_Summary.txt\"\n",
        "    with open(summary_filename, 'a') as f:\n",
        "        for line in summary_lines:\n",
        "            f.write(line + '\\n')\n",
        "    print(f\"[{sample_id}] Summary written: {summary_filename}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}